#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import pickle
import warnings
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from pyscenic.utils import modules_from_adjacencies
from pyscenic.prune import prune2df, df2regulons
from pyscenic.aucell import aucell
from ctxcore.rnkdb import FeatherRankingDatabase as RankingDatabase
from arboreto.algo import grnboost2
from arboreto.utils import load_tf_names
from dask.distributed import Client, LocalCluster
import psutil
import gc
import time
from ctxcore.genesig import Regulon
import dask.config

warnings.filterwarnings('ignore')
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# -------------------- è·¯å¾„é…ç½® --------------------
EXPR_PATH = r'/data/wuwei/cell/BERT/case_study/HNSCC/expr_matrix.csv'
META_PATH = r'/data/wuwei/cell/BERT/case_study/HNSCC/cell_annotations.csv'
LR_PATH = r'/data/wuwei/cell/BERT/out/five-fold-cross-validation/auc/dataset1/predictions+known_lri.csv'
TF_LIST_PATH = r'/data/wuwei/cell/BERT/dataset/tf_motif_rankDB/allTFs_hg38.csv'
FEATHER_PATH = r'/data/wuwei/cell/BERT/dataset/tf_motif_rankDB/hg38__refseq-r80__10kb_up_and_down_tss.mc9nr.genes_vs_motifs.rankings.feather'
MOTIF_ANNO = r'/data/wuwei/cell/BERT/dataset/tf_motif_rankDB/motifs-v9-nr.hgnc-m0.001-o0.0.tbl'

# -------------------- å‚æ•°é…ç½® --------------------
THRESH_GENES = 500
THRESH_COUNTS = 1000
THRESH_LR_TF = 0.8
MIN_TF_ACTIVITY = 0.1
COMMUNICATION_THRESHOLD = 0.15
MEMORY_LIMIT = "40GB"
NUM_WORKERS = 4
MIN_GENE_EXPRESSION = 0.1
MIN_CELLS_WITH_GENE = 10
MIN_CELL_PERCENT = 0.05
MIN_EXPRESSION_PERCENTILE = 10
MIN_REGULON_COVERAGE = 0.5
MAX_REGULON_GENES = 100
MIN_REGULON_GENES = 5

def optimize_memory(df):
    for col in df.columns:
        if pd.api.types.is_float_dtype(df[col]):
            df[col] = df[col].astype('float32')
        elif pd.api.types.is_integer_dtype(df[col]):
            df[col] = pd.to_numeric(df[col], downcast='unsigned')
        elif pd.api.types.is_object_dtype(df[col]):
            df[col] = df[col].astype('category')
    return df

def load_and_validate_inputs():
    """åŠ è½½å¹¶æ ¡éªŒè¾“å…¥æ•°æ®ï¼Œè¿”å› expr, meta, lr"""
    print("ğŸ§ª åŠ è½½æ•°æ®...")
    expr = pd.read_csv(EXPR_PATH, index_col=0, skiprows=[1, 2, 3, 4, 5])
    expr.index = expr.index.str.strip("'")
    expr = optimize_memory(expr.astype(float))

    meta = pd.read_csv(META_PATH, index_col=0, usecols=['Cell', 'classified as cancer cell', 'non-cancer cell type'])
    meta = optimize_memory(meta)

    lr = pd.read_csv(LR_PATH)
    lr = optimize_memory(lr)

    print(f"âœ… åŸå§‹æ•°æ® | åŸºå› : {expr.shape[0]} | ç»†èƒ: {expr.shape[1]}")
    print(f"âœ… å…ƒæ•°æ®   | ç»†èƒ: {meta.shape[0]}")
    print(f"âœ… LRå¯¹     | æ¡ç›®: {lr.shape[0]}")

    # ç¡®ä¿ç´¢å¼•å¯¹é½
    if not expr.columns.equals(meta.index):
        print("âš ï¸ è¡¨è¾¾çŸ©é˜µä¸å…ƒæ•°æ®çš„ cell é¡ºåºä¸ä¸€è‡´ï¼Œå°è¯•å¯¹é½...")
        common_cells = expr.columns.intersection(meta.index)
        expr = expr[common_cells]
        meta = meta.loc[common_cells]
        print(f"å¯¹é½å | åŸºå› : {expr.shape[0]} | ç»†èƒ: {expr.shape[1]}")

    # æ£€æŸ¥ç©ºæ•°æ®
    if expr.empty or meta.empty:
        raise ValueError("âŒ è¡¨è¾¾çŸ©é˜µæˆ–å…ƒæ•°æ®ä¸ºç©ºï¼")

    return expr, meta, lr

def filter_genes_debug(expr):
    print("ğŸ§¬ è¿›è¡ŒåŸºå› è¿‡æ»¤...")
    gene_means = expr.mean(axis=1)
    gene_n_cells = (expr > 0).sum(axis=1)
    total_cells = expr.shape[1]
    min_cells = max(50, int(total_cells * MIN_CELL_PERCENT))
    mean_threshold = np.percentile(gene_means, MIN_EXPRESSION_PERCENTILE)
    print(f"è¿‡æ»¤é˜ˆå€¼ | æœ€å°ç»†èƒæ•°: {min_cells} | å¹³å‡è¡¨è¾¾åˆ†ä½æ•°: {mean_threshold:.4f}")
    good_genes = (gene_means >= mean_threshold) & (gene_n_cells >= min_cells)
    expr_filt = expr.loc[good_genes]
    print(f"ä¿ç•™åŸºå› : {expr_filt.shape[0]}/{expr.shape[0]} ({expr_filt.shape[0]/expr.shape[0]*100:.1f}%)")
    if expr_filt.empty:
        raise ValueError("âŒ åŸºå› è¿‡æ»¤åæ²¡æœ‰åŸºå› å‰©ä½™ï¼")
    return expr_filt

def filter_cells_debug(expr, meta):
    print("ğŸ” ç»†èƒè´¨æ§...")
    n_genes = (expr > 0).sum(axis=0)
    n_counts = expr.sum(axis=0)
    qc_metrics = pd.DataFrame({'n_genes': n_genes, 'n_counts': n_counts})
    qc_metrics.to_csv('cell_qc_metrics.csv')
    good_cells = (n_genes >= THRESH_GENES) & (n_counts >= THRESH_COUNTS)
    good_cells = good_cells.reindex(meta.index, fill_value=False)
    expr_filt = expr.loc[:, good_cells]
    meta_filt = meta.loc[good_cells]
    if expr_filt.empty:
        raise ValueError("âŒ ç»†èƒè¿‡æ»¤åæ²¡æœ‰ç»†èƒå‰©ä½™ï¼")
    expr_filt = filter_genes_debug(expr_filt)
    expr_filt = expr_filt.astype('float32')
    print(f"âœ… è´¨æ§å®Œæˆ | ç»†èƒ: {expr_filt.shape[1]} | åŸºå› : {expr_filt.shape[0]}")
    return expr_filt, meta_filt

def prepare_grn_inputs(expr_filt):
    tf_names_raw = load_tf_names(TF_LIST_PATH)
    tf_names = [str(tf).strip().strip("'").upper() for tf in tf_names_raw]
    gene_names = [str(g).strip().strip("'").upper() for g in expr_filt.index]
    expr_filt.index = gene_names  # è¦†ç›–åŸå§‹ç´¢å¼•
    common_tfs = [tf for tf in tf_names if tf in gene_names]
    print(f"âœ… TFåˆ—è¡¨ | åŸå§‹: {len(tf_names)} | å­˜åœ¨äºè¡¨è¾¾çŸ©é˜µ: {len(common_tfs)}")
    if not common_tfs:
        print("â— å‰10ä¸ªè¡¨è¾¾çŸ©é˜µåŸºå› ç¤ºä¾‹:", gene_names[:10])
        print("â— å‰10ä¸ªTFç¤ºä¾‹:", tf_names[:10])
        raise ValueError("âŒ æ²¡æœ‰TFå­˜åœ¨äºè¡¨è¾¾çŸ©é˜µä¸­ï¼Œè¯·æ£€æŸ¥åŸºå› åæ ¼å¼ï¼")
    return common_tfs, gene_names

def main():
    total_ram = psutil.virtual_memory().total // (1024 ** 3)
    print(f"ç³»ç»Ÿèµ„æº | CPU: {psutil.cpu_count()} | å†…å­˜: {total_ram}GB")

    # 1. åŠ è½½å’Œæ ¡éªŒ
    expr, meta, lr = load_and_validate_inputs()
    expr_filt, meta_filt = filter_cells_debug(expr, meta)
    del expr, meta
    gc.collect()

    # 2. å‡†å¤‡ GRN è¾“å…¥
    common_tfs, gene_names = prepare_grn_inputs(expr_filt)

    # 3. å¯åŠ¨ Dask
    cluster = LocalCluster(n_workers=NUM_WORKERS, threads_per_worker=1,
                           memory_limit=MEMORY_LIMIT, silence_logs=30)
    client = Client(cluster)
    print(f"âœ… Daskå·²å¯åŠ¨ | ä»ªè¡¨ç›˜: {cluster.dashboard_link}")

    print("âœ… åŸºå› åæ ¼å¼æ£€æŸ¥ | å‰5ä¸ªåŸºå› :", expr_filt.index[:5])
    print("âœ… TFåæ ¼å¼æ£€æŸ¥   | å‰5ä¸ªTF:", common_tfs[:5])

    try:
        # 4. GRN æ¨æ–­
        print("ğŸ§¬ å¼€å§‹ GRN æ¨æ–­...")
        expr_clean = expr_filt.astype(float).values
        print(f"ä¼ å…¥ grnboost2 çš„ TF æ•°é‡: {len(common_tfs)}")
        print(f"ä¼ å…¥ grnboost2 çš„åŸºå› æ•°é‡: {len(expr_filt.index)}")

        print("ğŸ§ª æ£€æŸ¥è¡¨è¾¾æ–¹å·®...")
        variances = expr_filt.var(axis=1)
        print(f"æœ€å°æ–¹å·®: {variances.min():.6f}")
        print(f"æœ€å¤§æ–¹å·®: {variances.max():.6f}")
        print(f"é›¶æ–¹å·®åŸºå› æ•°: {(variances == 0).sum()}")

        if variances.max() < 1e-6:
            raise ValueError("âŒ æ‰€æœ‰åŸºå› è¡¨è¾¾æ–¹å·®è¿‡ä½ï¼Œæ— æ³•å»ºæ¨¡ï¼")

        # å¼ºåˆ¶æ£€æŸ¥ NaN/Inf
        expr_filt = expr_filt.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')
        expr_filt = expr_filt.astype(np.float32)

        print("âœ… æ¸…ç† NaN/Inf å | åŸºå› : {} | ç»†èƒ: {}".format(expr_filt.shape[0], expr_filt.shape[1]))
        print("æ˜¯å¦æœ‰ NaN:", np.isnan(expr_filt.values).any())
        print("æ˜¯å¦æœ‰ Inf:", np.isinf(expr_filt.values).any())

        adj = grnboost2(
            expression_data=expr_filt.T.astype(float),
            gene_names=expr_filt.index.astype(str).tolist(),
            tf_names=common_tfs,
            client_or_address=client,
            verbose=True,
            seed=42,
        )
        print(f"âœ… GRN æ¨æ–­å®Œæˆ | è¾¹æ•°: {adj.shape[0]}")

        # 5. å…¶ä½™æ­¥éª¤çœç•¥ï¼ˆä¸åŸå§‹è„šæœ¬ç›¸åŒï¼‰
        feather_db = RankingDatabase(FEATHER_PATH, 'hg38')
        modules = modules_from_adjacencies(adj, expr_filt.T)
        regulons = []
        total_mod = len(modules)
        for i, mod in enumerate(modules):
            try:
                df = prune2df([feather_db], [mod], MOTIF_ANNO, client_or_address=client)
                if not df.empty:
                    regs = df2regulons(df)
                    for reg in regs:
                        genes = list(reg.genes)[:MAX_REGULON_GENES]
                        present = [g for g in genes if g in expr_filt.index]
                        cov = len(present) / len(genes) if genes else 0
                        if cov >= MIN_REGULON_COVERAGE and len(present) >= MIN_REGULON_GENES:
                            new_reg = Regulon(
                                reg.name,
                                reg.gene2weight,
                                reg.transcription_factor,
                                reg.context
                            )
                            regulons.append(new_reg)

            except Exception as e:
                print(f"æ¨¡å— {i} å¤„ç†å¤±è´¥: {e}")

            # ===== æ¯ 50 ä¸ªæ¨¡å—æ‰‹åŠ¨å›æ”¶ä¸€æ¬¡ =====
            if (i + 1) % 50 == 0:
                gc.collect()
                client.run(lambda: gc.collect())  # æ‰€æœ‰ worker ä¸€èµ·æ”¶
                print(f"â†» å·²å¤„ç† {i + 1}/{total_mod} æ¨¡å—ï¼Œå¼ºåˆ¶ GC ä¸€æ¬¡")

        print(f"âœ… è°ƒæ§å­æ„å»ºå®Œæˆ | åŸå§‹: {len(modules)} | æœ‰æ•ˆ: {len(regulons)}")
        auc_mtx = aucell(expr_filt.T, regulons, auc_threshold=0.05, noweights=True, num_workers=NUM_WORKERS)
        auc_mtx.to_csv('auc_mtx.csv')
        print(f"âœ… TFæ´»æ€§è®¡ç®—å®Œæˆ | TF: {auc_mtx.shape[1]} | ç»†èƒ: {auc_mtx.shape[0]}")

        # -------------- å¯¼å‡º regulons.csv --------------
        regulons_df = pd.DataFrame([
            {"tf": reg.transcription_factor, "gene": g, "weight": w}
            for reg in regulons
            for g, w in reg.gene2weight.items()
        ])
        regulons_df.to_csv('regulons.csv', index=False)
        print("âœ… regulons.csv å·²å¯¼å‡ºå®Œæˆ")

        # -------------- ä¸€æ¬¡æ€§ä¿å­˜ä¸­é—´ç»“æœ --------------
        adj.to_pickle('adj.pkl')  # GRN è¾¹è¡¨
        with open('modules.pkl', 'wb') as f:
            pickle.dump(modules, f)  # æ¨¡å—åˆ—è¡¨
        with open('regulons.pkl', 'wb') as f:
            pickle.dump(regulons, f)  # regulon å¯¹è±¡
        print("âœ… å·²ä¿å­˜ adj.pkl / modules.pkl / regulons.pkl")

    except Exception as e:
        print("âŒ åˆ†æå‡ºé”™:", e)
        import traceback
        traceback.print_exc()
    finally:
        client.close()
        cluster.close()
        print("âœ… Daskå·²å…³é—­")

if __name__ == '__main__':
    dask.config.set({
        "distributed.worker.memory.target": 0.5,  # 50 % å¼€å§‹ spill
        "distributed.worker.memory.spill": 0.7,  # 70 % å¼ºåˆ¶ spill
        "distributed.worker.memory.pause": 0.98,  # 90 % æ‰æš‚åœ
        "distributed.worker.memory.terminate": 0.99  # 98 % æ‰ kill
    })
    main()
