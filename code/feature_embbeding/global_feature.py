import pandas as pd
import torch
import re
import os
import numpy as np
from transformers import T5Tokenizer, T5EncoderModel
from tqdm import tqdm
import time
import sys

# ==== è®¾ç½®ç¯å¢ƒ ====
print("ğŸ“¦ è›‹ç™½è´¨ç‰¹å¾æå–ç¨‹åºå¯åŠ¨...")
start_time = time.time()

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

# è®¾ç½®æ¨¡å‹è·¯å¾„
MODEL_PATH = "../../PorstT5"
print(f"ğŸ” æ¨¡å‹è·¯å¾„: {os.path.abspath(MODEL_PATH)}")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
if not os.path.exists(MODEL_PATH):
    print(f"âŒ é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_PATH}")
    print("è¯·ç¡®è®¤æ¨¡å‹ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
    sys.exit(1)

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
model_files = os.listdir(MODEL_PATH)
required_files = ["config.json", "pytorch_model.bin", "tokenizer_config.json"]
missing_files = [f for f in required_files if f not in model_files]

if missing_files:
    print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œç¼ºå°‘æ–‡ä»¶: {', '.join(missing_files)}")
    print("è¯·é‡æ–°ä¸‹è½½å®Œæ•´æ¨¡å‹")
    sys.exit(1)
else:
    print("âœ… æ¨¡å‹æ–‡ä»¶å®Œæ•´")

# ==== åŠ è½½æ¨¡å‹ ====
print("\nğŸš€ æ­£åœ¨åŠ è½½ProstT5æ¨¡å‹...")

# å°è¯•åŠ è½½tokenizer
try:
    print("  1. åŠ è½½tokenizer...")
    tokenizer = T5Tokenizer.from_pretrained(
        MODEL_PATH,
        do_lower_case=False,
        legacy=False  # ç¦ç”¨æ—§ç‰ˆè¡Œä¸º
    )
    print("  âœ… tokenizeråŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"  âŒ tokenizeråŠ è½½å¤±è´¥: {str(e)}")
    print("  å°è¯•ä½¿ç”¨æ—§ç‰ˆtokenizer...")
    tokenizer = T5Tokenizer.from_pretrained(
        MODEL_PATH,
        do_lower_case=False,
        legacy=True
    )
    print("  âœ… æ—§ç‰ˆtokenizeråŠ è½½æˆåŠŸ")

# å°è¯•åŠ è½½ä¸»æ¨¡å‹
try:
    print("  2. åŠ è½½ä¸»æ¨¡å‹...")
    model_encoder = T5EncoderModel.from_pretrained(MODEL_PATH).to(device)
    print(f"  âœ… ä¸»æ¨¡å‹åŠ è½½æˆåŠŸ ({model_encoder.__class__.__name__})")
except Exception as e:
    print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    sys.exit(1)

# è®¾ç½®æ¨¡å‹ç²¾åº¦
if device.type == 'cuda':
    model_encoder = model_encoder.half()
    print("  ğŸ”„ æ¨¡å‹è®¾ç½®ä¸ºåŠç²¾åº¦ (float16)")
else:
    print("  ğŸ”„ CPUæ¨¡å¼ä½¿ç”¨å•ç²¾åº¦ (float32)")

# æµ‹è¯•æ¨¡å‹æ¨ç†
print("  3. æµ‹è¯•æ¨¡å‹æ¨ç†...")
try:
    test_seq = "MKKLLFAIVLVVLTLLTAEGPSQPTR"
    inputs = tokenizer(
        f"<AA2fold> {' '.join(list(test_seq))}",
        return_tensors="pt",
        add_special_tokens=True,
        padding=True
    ).to(device)

    with torch.no_grad():
        output = model_encoder(**inputs)

    print(f"  âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {output.last_hidden_state.shape}")
except Exception as e:
    print(f"  âŒ æ¨¡å‹æ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
    sys.exit(1)

# ==== è®¾ç½®æ•°æ®è·¯å¾„ ====
DATA_DIR = "./dataset1"
LIGAND_FILE = os.path.join(DATA_DIR, "ligand.csv")
RECEPTOR_FILE = os.path.join(DATA_DIR, "receptor.csv")
OUTPUT_DIR = os.path.join(DATA_DIR, "features_1")

os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"\nğŸ“‚ æ•°æ®æ–‡ä»¶è·¯å¾„:")
print(f"  é…ä½“æ•°æ®: {os.path.abspath(LIGAND_FILE)}")
print(f"  å—ä½“æ•°æ®: {os.path.abspath(RECEPTOR_FILE)}")
print(f"  è¾“å‡ºç›®å½•: {os.path.abspath(OUTPUT_DIR)}")

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
for path in [LIGAND_FILE, RECEPTOR_FILE]:
    if not os.path.exists(path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {os.path.abspath(path)}")
        sys.exit(1)


# ==== æ ¸å¿ƒåŠŸèƒ½ ====
def preprocess_sequence(seq):
    """é¢„å¤„ç†è›‹ç™½è´¨åºåˆ—"""
    seq = re.sub(r"[UZOB]", "X", seq.upper())
    return " ".join(list(seq))


def extract_global_features(seq, max_length=1024):
    """æå–è›‹ç™½è´¨çš„å…¨å±€ç‰¹å¾å‘é‡"""
    # å¤„ç†è¶…é•¿åºåˆ—
    original_seq = seq
    if len(seq) > max_length:
        seq = seq[:max_length] + "..."  # æˆªæ–­ä½†ä¿ç•™åç¼€è¡¨ç¤º

    # é¢„å¤„ç†åºåˆ—
    processed = preprocess_sequence(seq)

    # æ·»åŠ å‰ç¼€ï¼ˆAAåˆ°3Diï¼‰
    full_seq = "<AA2fold> " + processed

    # ç¼–ç è¾“å…¥
    inputs = tokenizer(
        full_seq,
        return_tensors="pt",
        add_special_tokens=True,
        padding='max_length',
        max_length=min(1024, len(processed) + 10),  # åºåˆ—é•¿åº¦+ç¼“å†²
        truncation=True
    ).to(device)

    # æå–åµŒå…¥
    with torch.no_grad():
        output = model_encoder(**inputs)

    # è·å–æ‰€æœ‰éå¡«å……tokençš„åµŒå…¥
    embeddings = output.last_hidden_state[0]

    # è®¡ç®—å…¨å±€ç‰¹å¾ï¼ˆå‡å€¼æ± åŒ–ï¼‰
    global_feature = embeddings.mean(dim=0).cpu().numpy()

    return global_feature

def process_protein_file(file_path, prefix):
    """å¤„ç†è›‹ç™½è´¨æ–‡ä»¶å¹¶æå–ç‰¹å¾"""
    print(f"\nğŸ” å¼€å§‹å¤„ç† {prefix} æ•°æ®: {os.path.basename(file_path)}")

    # è¯»å–CSVæ–‡ä»¶
    try:
        print(f"  è¯»å–æ•°æ®æ–‡ä»¶...")
        # å‡è®¾æ–‡ä»¶æœ‰è¡¨å¤´ï¼šprotein_name å’Œ sequence
        df = pd.read_csv(file_path)
        print(f"  âœ… è¯»å–å®Œæˆ! å…± {len(df)} æ¡è›‹ç™½è´¨åºåˆ—")
        print(f"  æ ·æœ¬æ•°æ®: {df.iloc[0]['protein_name']} - {df.iloc[0]['sequence'][:30]}...")
    except Exception as e:
        print(f"  âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        print(f"  å°è¯•æ— è¡¨å¤´è¯»å–...")
        try:
            df = pd.read_csv(file_path, header=None, names=["protein_name", "sequence"])
            print(f"  âœ… æ— è¡¨å¤´è¯»å–æˆåŠŸ! å…± {len(df)} æ¡åºåˆ—")
        except:
            print(f"  âŒ æ— æ³•è¯»å–æ–‡ä»¶! è·³è¿‡å¤„ç†ã€‚")
            return

    # ç¡®ä¿åºåˆ—æ˜¯å­—ç¬¦ä¸²
    df['sequence'] = df['sequence'].astype(str)

    # å‡†å¤‡ç»“æœå­˜å‚¨
    global_features = []
    protein_names = []
    error_count = 0
    processed_count = 0

    # è¿›åº¦æ¡è®¾ç½®
    print("  â³ å¼€å§‹ç‰¹å¾æå–...")
    progress_bar = tqdm(df.iterrows(), total=len(df), desc=f"æå–{prefix}ç‰¹å¾")

    # å¤„ç†æ¯æ¡åºåˆ—
    for idx, row in progress_bar:
        name = row["protein_name"]
        seq = row["sequence"]

        protein_names.append(name)
        progress_bar.set_postfix({
            "è›‹ç™½è´¨": name[:10] + "..." if len(name) > 10 else name,
            "é•¿åº¦": len(seq),
            "é”™è¯¯": error_count
        })

        try:
            # å¯¹éå¸¸é•¿çš„åºåˆ—è¿›è¡Œè­¦å‘Š
            if len(seq) > 1500:
                print(f"\n  âš ï¸ è­¦å‘Š: {name} åºåˆ—è¶…é•¿ ({len(seq)}ä¸ªæ®‹åŸº)")

            # æå–ç‰¹å¾
            feat = extract_global_features(seq)
            global_features.append(feat)
            processed_count += 1

        except torch.cuda.OutOfMemoryError:
            print(f"\n  â—ï¸ CUDAæ˜¾å­˜ä¸è¶³! è·³è¿‡è›‹ç™½è´¨ {name} (é•¿åº¦: {len(seq)})")
            global_features.append(np.zeros(1024))
            error_count += 1
            # GPUå†…å­˜æ¸…ç†
            if device.type == 'cuda':
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"\n  â—ï¸ å¤„ç†{name}æ—¶å‡ºé”™: {str(e)}")
            global_features.append(np.zeros(1024))
            error_count += 1

        # è¿›åº¦æŠ¥å‘Š
        if processed_count % 10 == 0:
            # æ˜¾å­˜ä½¿ç”¨æƒ…å†µæŠ¥å‘Š
            if device.type == 'cuda':
                mem_used = torch.cuda.memory_allocated() / 1024 ** 2
                mem_total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2
                progress_bar.set_postfix({
                    "è¿›åº¦": f"{idx + 1}/{len(df)}",
                    "GPUå†…å­˜": f"{mem_used:.0f}/{mem_total:.0f} MB"
                })

    # åˆ›å»ºDataFrameä¿å­˜ç»“æœ
    result_df = pd.DataFrame(global_features)
    result_df.insert(0, "protein_name", protein_names)

    # åˆ—åæ ¼å¼åŒ–
    feature_columns = [f"feat_{i}" for i in range(1024)]
    result_df.columns = ["protein_name"] + feature_columns

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, f"{prefix}_global_features.csv")
    result_df.to_csv(output_path, index=False)
    print(f"\nâœ… {prefix}å…¨å±€ç‰¹å¾å·²ä¿å­˜: {os.path.basename(output_path)}")
    print(f"  å¤„ç†å®Œæˆ! æ€»åºåˆ—: {len(df)}, æˆåŠŸ: {processed_count}, å¤±è´¥: {error_count}")

    return result_df


# ==== ä¸»ç¨‹åº ====
if __name__ == "__main__":
    print("\n" + "=" * 50)

    # å…ˆå¤„ç†ä¸€ä¸ªæ ·æœ¬æµ‹è¯•
    print("\nğŸ§ª è¿è¡Œæµ‹è¯•æ ·æœ¬...")
    test_protein = "ENSP00000002829"
    test_sequence = "MLVAGLLLSVLSLTAIGAPSPTQODLIPATPVPVLSFLSKELKATCTAIPFFHLQLTYLLTLLKGDHRMDVYGCSKVDYLSLSLIDHNEEPPLIHHWAAAGTGGTGGKDSKVDYLSKFLVSLKFDGLVSL"

    try:
        print(f"  è›‹ç™½è´¨: {test_protein}")
        print(f"  åºåˆ—é•¿åº¦: {len(test_sequence)}")

        test_feature = extract_global_features(test_sequence)
        print(f"  âœ… æµ‹è¯•æˆåŠŸ! ç‰¹å¾å‘é‡é•¿åº¦: {len(test_feature)}")

    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("  è¯·åœ¨è§£å†³æ­¤é—®é¢˜åå†ç»§ç»­å¤„ç†ä¸»æ–‡ä»¶")
        sys.exit(1)

    # å¤„ç†ä¸»æ–‡ä»¶
    print("\n" + "=" * 50)
    print("ğŸš€ å¼€å§‹å¤„ç†ä¸»æ–‡ä»¶...")

    ligand_result = process_protein_file(LIGAND_FILE, "ligand")

    print("\n" + "=" * 50)

    receptor_result = process_protein_file(RECEPTOR_FILE, "receptor")

    # å®Œæˆç»Ÿè®¡
    total_time = time.time() - start_time
    print("\n" + "=" * 50)
    print(f"ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆ! æ€»ç”¨æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")

    if ligand_result is not None:
        print(f"é…ä½“æ•°é‡: {len(ligand_result)}")
    if receptor_result is not None:
        print(f"å—ä½“æ•°é‡: {len(receptor_result)}")

    # GPUå†…å­˜ä½¿ç”¨ç»Ÿè®¡
    if device.type == 'cuda':
        print(f"å³°å€¼GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024 ** 2:.2f} MB")
